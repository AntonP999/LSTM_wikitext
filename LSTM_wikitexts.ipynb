{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, BPTTIterator\n",
    "from torchtext.datasets import WikiText2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a recurrent neural net of LSTM units is used for prediction of next items in sequences of text data from wikitext corpus. The text sequences here split to single symbols, so the task is to predict what symbol should go next based on several previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length:  245\n",
      "[('<unk>', 0), ('<pad>', 1), ('<eos>', 2), (' ', 3), ('e', 4), ('t', 5), ('a', 6), ('n', 7), ('i', 8), ('o', 9), ('r', 10), ('s', 11), ('h', 12), ('d', 13), ('l', 14), ('u', 15), ('c', 16), ('m', 17), ('f', 18), ('g', 19), ('p', 20), ('w', 21), ('b', 22), ('y', 23), ('k', 24), (',', 25), ('.', 26), ('v', 27), ('<', 28), ('>', 29)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for splitting texts.\n",
    "tokenize = lambda x: re.findall(\".\", x)\n",
    "# torchtext.data.Field variable, necessary for preprocessing.\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, eos_token=\"<eos>\", lower=True)\n",
    "# Splitting WikiText2 dataset to train, validation and test sets.\n",
    "train, valid, test = WikiText2.splits(TEXT)\n",
    "# Building of vocabulary for embeddings.\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")\n",
    "# Vocabulary check. Each symbol assigned an id.\n",
    "print(\"Vocabulary length: \", len(list(TEXT.vocab.stoi.items())))\n",
    "print(list(TEXT.vocab.stoi.items())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables initializing. Length of sequence set to 30.\n",
    "batch_size = 128\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes BPTTIterator for splitting corpus to sequential batches with target, shifted by 1.\n",
    "train_iter, valid_iter, test_iter = BPTTIterator.splits((train, valid, test),\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         bptt_len=sequence_length,    \n",
    "                                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More variables initializing.\n",
    "eval_batch_size = 128\n",
    "grad_clip = 0.1\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "ntokens = weight_matrix.shape[0] # Number of tokens for embedding.\n",
    "nfeatures = weight_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5, lnorm=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # Shape: (vocabulary length, number of features).\n",
    "        self.lnorm = None\n",
    "        if lnorm:\n",
    "            self.lnorm = nn.LayerNorm(ninp)        \n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)  # Shape: (number of features, hidden state units, layers).        \n",
    "        self.decoder = nn.Linear(nhid, ntoken) # Conversion to vocabulary tokens for final multilabel classification task.\n",
    "        self.init_weights()        \n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):        \n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x)) # Output shape: (sequence length, batch size, number of features)\n",
    "        if self.lnorm is not None:\n",
    "            emb = self.lnorm(emb)        \n",
    "        output, hidden = self.rnn(emb, hidden) \n",
    "        # output shape: (sequence length, batch size, hidden size)\n",
    "        # hidden shape: (2 * (number of layers), batch size, hidden size). 1st for hidden state, 2nd for cell state.\n",
    "        output = self.drop(output)\n",
    "        # decoder input shape: (batch size * sequence length, hidden size).\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        # decoder output shape: (batch size * sequence length, vocabulary length).\n",
    "        # returns: (sequence length, batch size, vocabulary length).\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):        \n",
    "        weight = next(self.parameters()).data        \n",
    "        return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                weight.new(self.nlayers, bsz, self.nhid).zero_())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function.\n",
    "def evaluate(data_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0        \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, batch_data in enumerate(data_iter):        \n",
    "        data, targets = batch_data.text, batch_data.target # data and targets from torchtext.data.BPTTIterator.        \n",
    "        output, hidden = model(data) # Net ouput.\n",
    "        output_flat = output.view(-1, ntokens)        \n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item() # Cumulative loss.    \n",
    "    return total_loss / len(data_iter) # returns mean loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function.\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0    \n",
    "    for batch, batch_data in enumerate(train_iter):\n",
    "        data, targets = batch_data.text, batch_data.target        \n",
    "        optimizer.zero_grad()        \n",
    "        output, hidden = model(data) # Net output.        \n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1)) # Loss and backprop.\n",
    "        loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip) # Clipping gradient to counter RNNs exploding gradient problem.        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging.\n",
    "        total_loss += loss.item()        \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print(\"| epoch {:3d} | {:5d}/{:5d} batches | loss {:5.2f} | ppl {:8.2f}\".format(\n",
    "                epoch, batch, len(train_iter), cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(ntokens, 128, 128, 2, 0.3, lnorm=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1e+2, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence generation function.\n",
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    # First random symbol.\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    # Making n length sequence.\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)        \n",
    "        s_weights = output.squeeze().data.div(temp).exp() # Gets distribution (with temperature) for next symbol.        \n",
    "        s_idx = torch.multinomial(s_weights, 1)[0] # Samples next symbol index.\n",
    "        x.data.fill_(s_idx)        \n",
    "        s = TEXT.vocab.itos[s_idx] # Index to symbol and appends sequence.\n",
    "        out.append(s)\n",
    "    # returns string.\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and sequence generation. First sequence is generated by untrained net, and consist of random symbols. After first epoch the model starts to produce real word looking samples. Cross entropy was used as loss function, and perplexity as measure of model quality.\n",
    "\n",
    "$ppl = 2^{-\\sum_i{p_i\\log{q_i}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ¡ต礮аgลbс⁄ī£ćãửκ殻³(čヴ♯µoṃþ〉²£~隊ァxッ攻ル*тcầuხ्яμắ”#კłử \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | loss  2.93 | ppl    18.79\n",
      "| epoch   1 |   200/ 2808 batches | loss  2.18 | ppl     8.87\n",
      "| epoch   1 |   300/ 2808 batches | loss  2.06 | ppl     7.86\n",
      "| epoch   1 |   400/ 2808 batches | loss  2.00 | ppl     7.37\n",
      "| epoch   1 |   500/ 2808 batches | loss  1.95 | ppl     7.03\n",
      "| epoch   1 |   600/ 2808 batches | loss  1.92 | ppl     6.82\n",
      "| epoch   1 |   700/ 2808 batches | loss  1.89 | ppl     6.64\n",
      "| epoch   1 |   800/ 2808 batches | loss  1.88 | ppl     6.52\n",
      "| epoch   1 |   900/ 2808 batches | loss  1.86 | ppl     6.44\n",
      "| epoch   1 |  1000/ 2808 batches | loss  1.85 | ppl     6.37\n",
      "| epoch   1 |  1100/ 2808 batches | loss  1.83 | ppl     6.24\n",
      "| epoch   1 |  1200/ 2808 batches | loss  1.83 | ppl     6.26\n",
      "| epoch   1 |  1300/ 2808 batches | loss  1.82 | ppl     6.16\n",
      "| epoch   1 |  1400/ 2808 batches | loss  1.80 | ppl     6.05\n",
      "| epoch   1 |  1500/ 2808 batches | loss  1.80 | ppl     6.05\n",
      "| epoch   1 |  1600/ 2808 batches | loss  1.80 | ppl     6.05\n",
      "| epoch   1 |  1700/ 2808 batches | loss  1.80 | ppl     6.03\n",
      "| epoch   1 |  1800/ 2808 batches | loss  1.79 | ppl     6.00\n",
      "| epoch   1 |  1900/ 2808 batches | loss  1.79 | ppl     6.02\n",
      "| epoch   1 |  2000/ 2808 batches | loss  1.78 | ppl     5.92\n",
      "| epoch   1 |  2100/ 2808 batches | loss  1.79 | ppl     5.97\n",
      "| epoch   1 |  2200/ 2808 batches | loss  1.78 | ppl     5.96\n",
      "| epoch   1 |  2300/ 2808 batches | loss  1.79 | ppl     5.97\n",
      "| epoch   1 |  2400/ 2808 batches | loss  1.76 | ppl     5.83\n",
      "| epoch   1 |  2500/ 2808 batches | loss  1.77 | ppl     5.85\n",
      "| epoch   1 |  2600/ 2808 batches | loss  1.77 | ppl     5.89\n",
      "| epoch   1 |  2700/ 2808 batches | loss  1.76 | ppl     5.81\n",
      "| epoch   1 |  2800/ 2808 batches | loss  1.75 | ppl     5.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.55 | valid ppl     4.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " work avoras women , character as success , and the \n",
      "\n",
      "Epoch time: 15.45520714521408\n",
      "| epoch   2 |   100/ 2808 batches | loss  1.77 | ppl     5.87\n",
      "| epoch   2 |   200/ 2808 batches | loss  1.75 | ppl     5.74\n",
      "| epoch   2 |   300/ 2808 batches | loss  1.75 | ppl     5.76\n",
      "| epoch   2 |   400/ 2808 batches | loss  1.75 | ppl     5.75\n",
      "| epoch   2 |   500/ 2808 batches | loss  1.74 | ppl     5.71\n",
      "| epoch   2 |   600/ 2808 batches | loss  1.75 | ppl     5.73\n",
      "| epoch   2 |   700/ 2808 batches | loss  1.74 | ppl     5.71\n",
      "| epoch   2 |   800/ 2808 batches | loss  1.74 | ppl     5.71\n",
      "| epoch   2 |   900/ 2808 batches | loss  1.74 | ppl     5.69\n",
      "| epoch   2 |  1000/ 2808 batches | loss  1.74 | ppl     5.70\n",
      "| epoch   2 |  1100/ 2808 batches | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  1200/ 2808 batches | loss  1.74 | ppl     5.69\n",
      "| epoch   2 |  1300/ 2808 batches | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  1400/ 2808 batches | loss  1.72 | ppl     5.58\n",
      "| epoch   2 |  1500/ 2808 batches | loss  1.72 | ppl     5.58\n",
      "| epoch   2 |  1600/ 2808 batches | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  1700/ 2808 batches | loss  1.73 | ppl     5.63\n",
      "| epoch   2 |  1800/ 2808 batches | loss  1.72 | ppl     5.61\n",
      "| epoch   2 |  1900/ 2808 batches | loss  1.74 | ppl     5.68\n",
      "| epoch   2 |  2000/ 2808 batches | loss  1.72 | ppl     5.60\n",
      "| epoch   2 |  2100/ 2808 batches | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  2200/ 2808 batches | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  2300/ 2808 batches | loss  1.73 | ppl     5.65\n",
      "| epoch   2 |  2400/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   2 |  2500/ 2808 batches | loss  1.72 | ppl     5.58\n",
      "| epoch   2 |  2600/ 2808 batches | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  2700/ 2808 batches | loss  1.72 | ppl     5.60\n",
      "| epoch   2 |  2800/ 2808 batches | loss  1.72 | ppl     5.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.52 | valid ppl     4.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 97 , used to year sil . bense social : he song mas \n",
      "\n",
      "Epoch time: 15.52126392920812\n",
      "| epoch   3 |   100/ 2808 batches | loss  1.74 | ppl     5.70\n",
      "| epoch   3 |   200/ 2808 batches | loss  1.72 | ppl     5.56\n",
      "| epoch   3 |   300/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   400/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   500/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   600/ 2808 batches | loss  1.71 | ppl     5.56\n",
      "| epoch   3 |   700/ 2808 batches | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |   800/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   900/ 2808 batches | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |  1000/ 2808 batches | loss  1.71 | ppl     5.51\n",
      "| epoch   3 |  1100/ 2808 batches | loss  1.71 | ppl     5.51\n",
      "| epoch   3 |  1200/ 2808 batches | loss  1.71 | ppl     5.54\n",
      "| epoch   3 |  1300/ 2808 batches | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |  1400/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   3 |  1500/ 2808 batches | loss  1.70 | ppl     5.46\n",
      "| epoch   3 |  1600/ 2808 batches | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  1700/ 2808 batches | loss  1.71 | ppl     5.51\n",
      "| epoch   3 |  1800/ 2808 batches | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  1900/ 2808 batches | loss  1.72 | ppl     5.56\n",
      "| epoch   3 |  2000/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |  2100/ 2808 batches | loss  1.71 | ppl     5.55\n",
      "| epoch   3 |  2200/ 2808 batches | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  2300/ 2808 batches | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |  2400/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |  2500/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |  2600/ 2808 batches | loss  1.71 | ppl     5.55\n",
      "| epoch   3 |  2700/ 2808 batches | loss  1.71 | ppl     5.51\n",
      "| epoch   3 |  2800/ 2808 batches | loss  1.70 | ppl     5.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.51 | valid ppl     4.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " osia became number east was least those drawlall b \n",
      "\n",
      "Epoch time: 15.591724054018657\n",
      "| epoch   4 |   100/ 2808 batches | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |   200/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   4 |   300/ 2808 batches | loss  1.70 | ppl     5.47\n",
      "| epoch   4 |   400/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |   500/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |   600/ 2808 batches | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |   700/ 2808 batches | loss  1.70 | ppl     5.47\n",
      "| epoch   4 |   800/ 2808 batches | loss  1.70 | ppl     5.49\n",
      "| epoch   4 |   900/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1000/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1100/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1200/ 2808 batches | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1300/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1400/ 2808 batches | loss  1.68 | ppl     5.38\n",
      "| epoch   4 |  1500/ 2808 batches | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  1600/ 2808 batches | loss  1.69 | ppl     5.45\n",
      "| epoch   4 |  1700/ 2808 batches | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |  1800/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1900/ 2808 batches | loss  1.71 | ppl     5.52\n",
      "| epoch   4 |  2000/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   4 |  2100/ 2808 batches | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  2200/ 2808 batches | loss  1.70 | ppl     5.49\n",
      "| epoch   4 |  2300/ 2808 batches | loss  1.71 | ppl     5.50\n",
      "| epoch   4 |  2400/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  2500/ 2808 batches | loss  1.69 | ppl     5.41\n",
      "| epoch   4 |  2600/ 2808 batches | loss  1.71 | ppl     5.51\n",
      "| epoch   4 |  2700/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  2800/ 2808 batches | loss  1.69 | ppl     5.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.49 | valid ppl     4.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  it was such as astant workments of cape of the fi \n",
      "\n",
      "Epoch time: 15.59484405517578\n",
      "| epoch   5 |   100/ 2808 batches | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |   200/ 2808 batches | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |   300/ 2808 batches | loss  1.69 | ppl     5.45\n",
      "| epoch   5 |   400/ 2808 batches | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |   500/ 2808 batches | loss  1.69 | ppl     5.45\n",
      "| epoch   5 |   600/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |   700/ 2808 batches | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |   800/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |   900/ 2808 batches | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1000/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1100/ 2808 batches | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  1200/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1300/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1400/ 2808 batches | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1500/ 2808 batches | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |  1600/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1700/ 2808 batches | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |  1800/ 2808 batches | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1900/ 2808 batches | loss  1.70 | ppl     5.49\n",
      "| epoch   5 |  2000/ 2808 batches | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  2100/ 2808 batches | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  2200/ 2808 batches | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  2300/ 2808 batches | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |  2400/ 2808 batches | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |  2500/ 2808 batches | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  2600/ 2808 batches | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |  2700/ 2808 batches | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  2800/ 2808 batches | loss  1.68 | ppl     5.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . friends division . <eos> the xeeciat ( 1130 ( <unk> \n",
      "\n",
      "Epoch time: 15.583664039770762\n"
     ]
    }
   ],
   "source": [
    "# Train, validation and samples output.\n",
    "with torch.no_grad():\n",
    "    print(\"sample:\\n\", generate(50), \"\\n\") # prints generated sample.\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    start_time = time()\n",
    "    train()\n",
    "    val_loss = evaluate(valid_iter)\n",
    "    print(\"-\" * 89)\n",
    "    print(\"| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}\".format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print(\"-\" * 89)    \n",
    "    with torch.no_grad():\n",
    "        print(\"sample:\\n\", generate(50), \"\\n\")\n",
    "    print(\"Epoch time: {}\".format((time()-start_time)/60.))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
